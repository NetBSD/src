$NetBSD: am-utils.info-5,v 1.1.1.5 1999/02/01 18:47:04 christos Exp $
This is Info file am-utils.info, produced by Makeinfo version 1.68 from
the input file am-utils.texi.

START-INFO-DIR-ENTRY
* Am-utils: (am-utils).          The Amd automounter suite of utilities
END-INFO-DIR-ENTRY


File: am-utils.info,  Node: Single-Host Mail Spool Directory,  Next: Centralized Mail Spool Directory,  Prev: Background to Mail Delivery,  Up: Background to Mail Delivery

Single-Host Mail Spool Directory
--------------------------------

   The most common method for mail delivery is for mail to be appended
to a mailbox file in a standard spool directory on the designated "mail
home" machine of the user. The greatest advantage of this method is
that it is the default method most vendors provide with their systems,
thus very little (if any) configuration is required on the SA's part.
All they need to set up are mail aliases directing mail to the host on
which the user's mailbox file is assigned.  (Otherwise, mail is
delivered locally, and users find mailboxes on many machines.)

   As users become more sophisticated, and aided by windowing systems,
they find themselves logging in on multiple hosts at once, performing
several tasks concurrently.  They ask to be able to read their mail on
any host on the network, not just the one designated as their "mail
home".


File: am-utils.info,  Node: Centralized Mail Spool Directory,  Next: Distributed Mail Spool Service,  Prev: Single-Host Mail Spool Directory,  Up: Background to Mail Delivery

Centralized Mail Spool Directory
--------------------------------

   A popular method for providing mail readability from any host is to
have all mail delivered to a mail spool directory on a designated
"mail-server" which is exported via NFS to all of the hosts on the
network.  Configuring such a system is relatively easy.  On most
systems, the bulk of the work is a one-time addition to one or two
configuration files in `/etc'.  The file-server's spool directory is
then hard-mounted across every machine on the local network.  In small
environments with only a handful of hosts this can be an acceptable
solution.  In our department, with a couple of hundred active hosts and
thousands of mail messages processed daily, this was deemed completely
unacceptable, as it introduced several types of problems:

Scalability and Performance
     As more and more machines get added to the network, more mail
     traffic has to go over NFS to and from the mail-server. Users like
     to run mail-watchers, and read their mail often. The stress on the
     shared infrastructure increases with every user and host added;
     loads on the mail server would most certainly be high since all
     mail delivery goes through that one machine.(1)  This leads to
     lower reliability and performance.  To reduce the number of
     concurrent connections between clients and the server host, some
     SAs have resorted to automounting the mail-spool directory.  But
     this solution only makes things worse: since users often run mail
     watchers, and many popular applications such as `trn', `emacs',
     `csh' or `ksh' check periodically for new mail, the automounted
     directory would be effectively permanently mounted.  If it gets
     unmounted automatically by the automounter program, it is most
     likely to get mounted shortly afterwards, consuming more I/O
     resources by the constant cycle of mount and umount calls.

Reliability
     The mail-server host and its network connectivity must be very
     reliable.  Worse, since the spool directory has to be
     hard-mounted,(2) many processes which access the spool directory
     (various shells, `login', `emacs', etc.)  would be hung as long as
     connectivity to the mail-server is severed. To improve
     reliability, SAs may choose to backup the mail-server's spool
     partition several times a day.  This may make things worse since
     reading or delivering mail while backups are in progress may cause
     backups to be inconsistent; more backups consume more backup-media
     resources, and increase the load on the mail-server host.

   ---------- Footnotes ----------

   (1)  Delivery via NFS-mounted filesystems may require usage of
`rpc.lockd' and `rpc.statd' to provide distributed file-locking, both
of which are widely regarded as unstable and unreliable.  Furthermore,
this will degrade performance, as local processes as well as remote
`nfsd' processes are kept busy.

   (2) No SA in their right minds would soft-mount read/write
partitions -- the chances for data loss are too great.


File: am-utils.info,  Node: Distributed Mail Spool Service,  Next: Why Deliver Into the Home Directory?,  Prev: Centralized Mail Spool Directory,  Up: Background to Mail Delivery

Distributed Mail Spool Service
------------------------------

   Despite the existence of a few systems that support delivery to
users' home directories, mail delivery to home directories hasn't
caught on.  We believe the main reason is that there are too many
programs that "know" where mailbox files reside.  Besides the obvious
(the delivery program `/bin/mail' and mail readers like `/usr/ucb/Mail',
`mush', `mm', etc.), other programs that know mailbox location are
login, from, almost every shell, `xbiff', `xmailbox', and even some
programs not directly related to mail, such as `emacs' and `trn'.
Although some of these programs can be configured to look in different
directories with the use of environment variables and other resources,
many of them cannot.  The overall porting work is significant.

   Other methods that have yet to catch on require the use of a special
mail-reading server, such as IMAP or POP.  The main disadvantage of
these systems is that UAs need to be modified to use these services --
a long and involved task.  That is why they are not popular at this
time.

   Several other ideas have been proposed and even used in various
environments.  None of them is robust.  They are mostly very
specialized, inflexible, and do not extend to the general case.  Some of
the ideas are plain bad, potentially leading to lost or corrupt mail:

automounters
     Using an automounter such as Amd to provide a set of symbolic links
     from the normal spool directory to user home directories is not
     sufficient.  UAs rename, unlink, and recreate the mailbox as a
     regular file, therefore it must be a real file, not a symbolic
     link.  Furthermore, it must reside in a real directory which is
     writable by the UAs and MTAs.  This method may also require
     populating `/var/spool/mail' with symbolic links and making sure
     they are updated.  Making Amd manage that directory directly
     fails, since many various lock files need to be managed as well.
     Also, Amd does not provide all of the NFS operations which are
     required to write mail such as write, create, remove, and unlink.

`$MAIL'
     Setting this variable to an automounted directory pointing to the
     user's mail spool host only solves the problem for those programs
     which know and use `$MAIL'.  Many programs don't, therefore this
     solution is partial and of limited flexibility.  Also, it requires
     the SAs or the users to set it themselves -- an added level of
     inconvenience and possible failures.

/bin/mail
     Using a different mail delivery agent could be the solution.  One
     such example is `hdmail'.  However, `hdmail' still requires
     modifying all UAs, the MTA's configuration, installing new
     daemons, and changing login scripts.  This makes the system less
     upgradable or compatible with others, and adds one more
     complicated system for SAs to deal with.  It is not a complete
     solution because it still requires each user have their `$MAIL'
     variable setup correctly, and that every program use this variable.


File: am-utils.info,  Node: Why Deliver Into the Home Directory?,  Prev: Distributed Mail Spool Service,  Up: Background to Mail Delivery

Why Deliver Into the Home Directory?
------------------------------------

   There are several major reasons why SAs might want to deliver mail
directly into the users' home directories:

Location
     Many mail readers need to move mail from the spool directory to the
     user's home directory.  It speeds up this operation if the two are
     on the same filesystem.  If for some reason the user's home
     directory is inaccessible, it isn't that useful to be able to read
     mail, since there is no place to move it to.  In some cases,
     trying to move mail to a non-existent or hung filesystem may
     result in mail loss.

Distribution
     Having all mail spool directories spread among the many more
     filesystems minimizes the chances that complete environments will
     grind to a halt when a single server is down.  It does increase
     the chance that there will be someone who is not able to read
     their mail when a machine is down, but that is usually preferred
     to having no one be able to read their mail because a centralized
     mail server is down.  The problem of losing some mail due to the
     (presumably) higher chances that a user's machine is down is
     minimized in HLFS.

Security
     Delivering mail to users' home directories has another advantage --
     enhanced security and privacy.  Since a shared system mail spool
     directory has to be world-readable and searchable, any user can see
     whether other users have mail, when they last received new mail,
     or when they last read their mail.  Programs such as `finger'
     display this information, which some consider an infringement of
     privacy.  While it is possible to disable this feature of `finger'
     so that remote users cannot see a mailbox file's status, this
     doesn't prevent local users from getting the information.
     Furthermore, there are more programs which make use of this
     information.  In shared environments, disabling such programs has
     to be done on a system-wide basis, but with mail delivered to
     users' home directories, users less concerned with privacy who do
     want to let others know when they last received or read mail can
     easily do so using file protection bits.

   In summary, delivering mail to home directories provides users the
functionality sought, and also avoids most of the problems just
discussed.


File: am-utils.info,  Node: Using Hlfsd,  Prev: Background to Mail Delivery,  Up: Hlfsd

Using Hlfsd
===========

* Menu:

* Controlling Hlfsd::
* Hlfsd Options::
* Hlfsd Files::


File: am-utils.info,  Node: Controlling Hlfsd,  Next: Hlfsd Options,  Prev: Using Hlfsd,  Up: Using Hlfsd

Controlling Hlfsd
-----------------

   Much the same way Amd is controlled by `ctl-amd', so does Hlfsd get
controlled by the `ctl-hlfsd' script:

ctl-hlfsd start
     Start a new Hlfsd.

ctl-hlfsd stop
     Stop a running Hlfsd.

ctl-hlfsd restart
     Stop a running Hlfsd, wait for 10 seconds, and then start a new
     one.  It is hoped that within 10 seconds, the previously running
     Hlfsd terminate properly; otherwise, starting a second one could
     cause system lockup.

   For example, on our systems, we start Hlfsd within `ctl-hlfsd' as
follows on Solaris 2 systems:

     hlfsd -a /var/alt_mail -x all -l /var/log/hlfsd /mail/home .mailspool

   The directory `/var/alt_mail' is a directory in the root partition
where alternate mail will be delivered into, when it cannot be delivered
into the user's home directory.

   Normal mail gets delivered into `/var/mail', but on our systems,
that is a symbolic link to `/mail/home'.  `/mail' is managed by Hlfsd,
which creates a dynamic symlink named `home', pointing to the
subdirectory `.mailspool' *within* the accessing user's home directory.
This results in mail which normally should go to `/var/mail/`$USER'',
to go to ``$HOME'/.mailspool/`$USER''.

   Hlfsd does not create the `/var/mail' symlink.  This needs to be
created (manually) once on each host, by the system administrators, as
follows:

     mv /var/mail /var/alt_mail
     ln -s /mail/home /var/mail


File: am-utils.info,  Node: Hlfsd Options,  Next: Hlfsd Files,  Prev: Controlling Hlfsd,  Up: Using Hlfsd

Hlfsd Options
-------------

-a ALT_DIR
     Alternate directory.  The name of the directory to which the
     symbolic link returned by Hlfsd will point, if it cannot access
     the home directory of the user.  This defaults to `/var/hlfs'.
     This directory will be created if it doesn't exist.  It is
     expected that either users will read these files, or the system
     administrators will run a script to resend this "lost mail" to its
     owner.

-c CACHE-INTERVAL
     Caching interval.  Hlfsd will cache the validity of home
     directories for this interval, in seconds.  Entries which have
     been verified within the last CACHE-INTERVAL seconds will not be
     verified again, since the operation could be expensive, and the
     entries are most likely still valid.  After the interval has
     expired, Hlfsd will re-verify the validity of the user's home
     directory, and reset the cache time-counter.  The default value
     for CACHE-INTERVAL is 300 seconds (5 minutes).

-f
     Force fast startup.  This option tells Hlfsd to skip startup-time
     consistency checks such as existence of mount directory, alternate
     spool directory, symlink to be hidden under the mount directory,
     their permissions and validity.

-g GROUP
     Set the special group HLFS_GID to GROUP.  Programs such as
     `/usr/ucb/from' or `/usr/sbin/in.comsat', which access the
     mailboxes of other users, must be setgid `HLFS_GID' to work
     properly.  The default group is `hlfs'.  If no group is provided,
     and there is no group `hlfs', this feature is disabled.

-h
     Help.  Print a brief help message, and exit.

-i RELOAD-INTERVAL
     Map-reloading interval.  Each RELOAD-INTERVAL seconds, Hlfsd will
     reload the password map.  Hlfsd needs the password map for the
     UIDs and home directory pathnames.  Hlfsd schedules a `SIGALRM' to
     reload the password maps.  A `SIGHUP' sent to Hlfsd will force it
     to reload the maps immediately.  The default value for
     RELOAD-INTERVAL is 900 seconds (15 minutes.)

-l LOGFILE
     Specify a log file to which Hlfsd will record events.  If LOGFILE
     is the string `syslog' then the log messages will be sent to the
     system log daemon by syslog(3), using the `LOG_DAEMON' facility.
     This is also the default.

-n
     No verify.  Hlfsd will not verify the validity of the symbolic link
     it will be returning, or that the user's home directory contains
     sufficient disk-space for spooling.  This can speed up Hlfsd at the
     cost of possibly returning symbolic links to home directories
     which are not currently accessible or are full.  By default, Hlfsd
     validates the symbolic-link in the background.  The `-n' option
     overrides the meaning of the `-c' option, since no caching is
     necessary.

-o MOUNT-OPTIONS
     Mount options which Hlfsd will use to mount itself on top of
     DIRNAME.  By default, MOUNT-OPTIONS is set to `ro'.  If the system
     supports symbolic-link caching, default options are set to
     `ro,nocache'.

-p
     Print PID.  Outputs the process-id of Hlfsd to standard output
     where it can be saved into a file.

-v
     Version.  Displays version information to standard error.

-x LOG-OPTIONS
     Specify run-time logging options.  The options are a comma
     separated list chosen from: `fatal', `error', `user', `warn',
     `info', `map', `stats', `all'.

-C
     Force Hlfsd to run on systems that cannot turn off the NFS
     attribute-cache.  Use of this option on those systems is
     discouraged, as it may result in loss or misdelivery of mail.  The
     option is ignored on systems that can turn off the attribute-cache.

-D LOG-OPTIONS
     Select from a variety of debugging options.  Prefixing an option
     with the string `no' reverses the effect of that option.  Options
     are cumulative.  The most useful option is `all'.  Since this
     option is only used for debugging other options are not documented
     here.  A fuller description is available in the program source.  A
     `SIGUSR1' sent to Hlfsd will cause it to dump its internal
     password map to the file `/usr/tmp/hlfsd.dump.XXXXXX', where
     `XXXXXX' will be replaced by a random string generated by
     mktemp(3) or (the more secure) mkstemp(3).

-P PASSWORD-FILE
     Read the user-name, user-id, and home directory information from
     the file PASSWORD-FILE.  Normally, Hlfsd will use getpwent(3) to
     read the password database.  This option allows you to override the
     default database, and is useful if you want to map users' mail
     files to a directory other than their home directory.  Only the
     username, uid, and home-directory fields of the file PASSWORD-FILE
     are read and checked.  All other fields are ignored.  The file
     PASSWORD-FILE must otherwise be compliant with Unix Version 7
     colon-delimited format passwd(4).


File: am-utils.info,  Node: Hlfsd Files,  Prev: Hlfsd Options,  Up: Using Hlfsd

Hlfsd Files
-----------

   The following files are used by Hlfsd:

`/hlfs'
     directory under which Hlfsd mounts itself and manages the symbolic
     link `home'.

`.hlfsdir'
     default sub-directory in the user's home directory, to which the
     `home' symbolic link returned by Hlfsd points.

`/var/hlfs'
     directory to which `home' symbolic link returned by Hlfsd points
     if it is unable to verify the that user's home directory is
     accessible.

   For discussion on other files used by Hlfsd, see *Note
lostaltmail::, and *Note lostaltmail.conf-sample::.


File: am-utils.info,  Node: Assorted Tools,  Next: Examples,  Prev: Hlfsd,  Up: Top

Assorted Tools
**************

   The following are additional utilities and scripts included with
am-utils, and get installed.

* Menu:

* am-eject::
* amd.conf-sample::
* amd2ldif::
* amd2sun::
* ctl-amd::
* ctl-hlfsd::
* expn::
* fix-amd-map::
* fixmount::
* fixrmtab::
* lostaltmail::
* lostaltmail.conf-sample::
* mk-amd-map::
* pawd::
* wait4amd::
* wait4amd2die::
* wire-test::


File: am-utils.info,  Node: am-eject,  Next: amd.conf-sample,  Prev: Assorted Tools,  Up: Assorted Tools

am-eject
========

   A shell script unmounts a floppy or CD-ROM that is automounted, and
then attempts to eject the removable device.


File: am-utils.info,  Node: amd.conf-sample,  Next: amd2ldif,  Prev: am-eject,  Up: Assorted Tools

amd.conf-sample
===============

   A sample Amd configuration file. *Note Amd Configuration File::.


File: am-utils.info,  Node: amd2ldif,  Next: amd2sun,  Prev: amd.conf-sample,  Up: Assorted Tools

amd2ldif
========

   A script to convert Amd maps to LDAP input files.  Use it as follows:

     amd2ldif mapname base < amd.mapfile > mapfile.ldif


File: am-utils.info,  Node: amd2sun,  Next: ctl-amd,  Prev: amd2ldif,  Up: Assorted Tools

amd2sun
=======

   A script to convert Amd maps to Sun Automounter maps.  Use it as
follows

     amd2sun < amd.mapfile > auto_mapfile


File: am-utils.info,  Node: ctl-amd,  Next: ctl-hlfsd,  Prev: amd2sun,  Up: Assorted Tools

ctl-amd
=======

   A script to start, stop, or restart Amd.  Use it as follows:

ctl-amd start
     Start a new Amd process.

ctl-amd stop
     Stop the running Amd.

ctl-amd restart
     Stop the running Amd (if any), safely wait for it to terminate, and
     then start a new process -- only if the previous one died cleanly.

   *Note Run-time Administration::, for more details.


File: am-utils.info,  Node: ctl-hlfsd,  Next: expn,  Prev: ctl-amd,  Up: Assorted Tools

ctl-hlfsd
=========

   A script for controlling Hlfsd, much the same way `ctl-amd' controls
Amd.  Use it as follows:

ctl-hlfsd start
     Start a new Hlfsd process.

ctl-hlfsd stop
     Stop the running Hlfsd.

ctl-hlfsd restart
     Stop the running Hlfsd (if any), wait for 10 seconds for it to
     terminate, and then start a new process -- only if the previous one
     died cleanly.

   *Note Hlfsd::, for more details.


File: am-utils.info,  Node: expn,  Next: fix-amd-map,  Prev: ctl-hlfsd,  Up: Assorted Tools

expn
====

   A script to expand email addresses into their full name.  It is
generally useful when using with the `lostaltmail' script, but is a
useful tools otherwise.

     $ expn -v ezk@cs.columbia.edu
     ezk@cs.columbia.edu ->
             ezk@shekel.mcl.cs.columbia.edu
     ezk@shekel.mcl.cs.columbia.edu ->
             Erez Zadok <"| /usr/local/mh/lib/slocal -user ezk || exit 75>
             Erez Zadok <\ezk>
             Erez Zadok </u/zing/ezk/.mailspool/backup>


File: am-utils.info,  Node: fix-amd-map,  Next: fixmount,  Prev: expn,  Up: Assorted Tools

fix-amd-map
===========

   Am-utils changed some of the syntax and default values of some
variables.  For example, the default value for `${os}' for Solaris 2.x
(aka SunOS 5.x) systems used to be `sos5', it is now more automatically
generated from `config.guess' and its value is `sunos5'.

   This script converts older Amd maps to new ones.  Use it as follows:

     fix-amd-map < old.map > new.map


File: am-utils.info,  Node: fixmount,  Next: fixrmtab,  Prev: fix-amd-map,  Up: Assorted Tools

fixmount
========

   `fixmount' is a variant of showmount(8) that can delete bogus mount
entries in remote mountd(8) daemons.  This is useful to cleanup
otherwise ever-accumulating "junk".  Use it for example:

     fixmount -r host

   See the online manual page for `fixmount' for more details of its
usage.


File: am-utils.info,  Node: fixrmtab,  Next: lostaltmail,  Prev: fixmount,  Up: Assorted Tools

fixrmtab
========

   A script to invalidate `/etc/rmtab' entries for hosts named.  Also
restart mountd for changes to take effect.  Use it for example:

     fixrmtab host1 host2 ...


File: am-utils.info,  Node: lostaltmail,  Next: lostaltmail.conf-sample,  Prev: fixrmtab,  Up: Assorted Tools

lostaltmail
===========

   A script used with Hlfsd to resend any "lost" mail.  Hlfsd redirects
mail which cannot be written into the user's home directory to an
alternate directory.  This is useful to continue delivering mail, even
if the user's file system was unavailable, full, or over quota.  But,
the mail which gets delivered to  the alternate directory needs to be
resent to its respective users.  This is what the `lostaltmail' script
does.

   Use it as follows:

     lostaltmail

   This script needs a configuration file `lostaltmail.conf' set up
with the right parameters to properly work.  *Note Hlfsd::, for more
details.


File: am-utils.info,  Node: lostaltmail.conf-sample,  Next: mk-amd-map,  Prev: lostaltmail,  Up: Assorted Tools

lostaltmail.conf-sample
=======================

   This is a text file with configuration parameters needed for the
`lostaltmail' script.  The script includes comments explaining each of
the configuration variables.  See it for more information.  Also *note
Hlfsd::. for general information.


File: am-utils.info,  Node: mk-amd-map,  Next: pawd,  Prev: lostaltmail.conf-sample,  Up: Assorted Tools

mk-amd-map
==========

   This program converts a normal Amd map file into an ndbm database
with the same prefix as the named file.  Use it as follows:

     mk-amd-map mapname


File: am-utils.info,  Node: pawd,  Next: wait4amd,  Prev: mk-amd-map,  Up: Assorted Tools

pawd
====

   Pawd is used to print the current working directory, adjusted to
reflect proper paths that can be reused to go through the automounter
for the shortest possible path.  In particular, the path printed back
does not include any of Amd's local mount points.  Using them is
unsafe, because Amd may unmount managed file systems from the mount
points, and thus including them in paths may not always find the files
within.

   Without any arguments, Pawd will print the automounter adjusted
current working directory.  With any number of arguments, it will print
the adjusted path of each one of the arguments.


File: am-utils.info,  Node: wait4amd,  Next: wait4amd2die,  Prev: pawd,  Up: Assorted Tools

wait4amd
========

   A script to wait for Amd to start on a particular host before
performing an arbitrary command.  The command is executed repeatedly,
with 1 second intervals in between.  You may interrupt the script using
`^C' (or whatever keyboard sequence your terminal's `intr' function is
bound to).

   Examples:

wait4amd saturn amq -p -h saturn
     When Amd is up on host `saturn', get the process ID of that
     running Amd.

wait4amd pluto rlogin pluto
     Remote login to host `pluto' when Amd is up on that host.  It is
     generally necessary to wait for Amd to properly start and
     initialize on a remote host before logging in to it, because
     otherwise user home directories may not be accessible across the
     network.

wait4amd pluto
     A short-hand version of the previous command, since the most useful
     reason for this script is to login to a remote host.  I use it very
     often when testing out new versions of Amd, and need to reboot hung
     hosts.


File: am-utils.info,  Node: wait4amd2die,  Next: wire-test,  Prev: wait4amd,  Up: Assorted Tools

wait4amd2die
============

   This script is used internally by `ctl-amd' when used to restart
Amd.  It waits for Amd to terminate.  If it detected that Amd
terminated cleanly, this script will return an exist status of zero.
Otherwise, it will return a non-zero exit status.

   The script tests for Amd's existence once every 5 seconds, six
times, for a total of 30 seconds.  It will return a zero exist status as
soon as it detects that Amd dies.


File: am-utils.info,  Node: wire-test,  Prev: wait4amd2die,  Up: Assorted Tools

wire-test
=========

   A simple program to test if some of the most basic networking
functions in am-util's library `libamu' work.  It also tests the
combination of NFS protocol and version number that are supported from
the current host, to a remote one.

   For example, in this test a machine which only supports NFS Version
2 is contacting a remote host that can support the same version, but
using both UDP and TCP.  If no host name is specified, `wire-test' will
try `localhost'.

     $ wire-test moisil
     Network name is "mcl-lab-net.cs.columbia.edu"
     Network number is "128.59.13"
     Network name is "old-net.cs.columbia.edu"
     Network number is "128.59.16"
     My IP address is 0x7f000001.
     NFS Version and protocol tests to host "moisil"...
             testing vers=2, proto="udp" -> found version 2.
             testing vers=3, proto="udp" -> failed!
             testing vers=2, proto="tcp" -> found version 2.
             testing vers=3, proto="tcp" -> failed!


File: am-utils.info,  Node: Examples,  Next: Internals,  Prev: Assorted Tools,  Up: Top

Examples
********

* Menu:

* User Filesystems::
* Home Directories::
* Architecture Sharing::
* Wildcard Names::
* rwho servers::
* /vol::
* /defaults with selectors::
* /tftpboot in a chroot-ed environment::


File: am-utils.info,  Node: User Filesystems,  Next: Home Directories,  Prev: Examples,  Up: Examples

User Filesystems
================

   With more than one fileserver, the directories most frequently
cross-mounted are those containing user home directories.  A common
convention used at Imperial College is to mount the user disks under
/home/machine.

   Typically, the `/etc/fstab' file contained a long list of entries
such as:

     machine:/home/machine /home/machine nfs ...

   for each fileserver on the network.

   There are numerous problems with this system.  The mount list can
become quite large and some of the machines may be down when a system is
booted.  When a new fileserver is installed, `/etc/fstab' must be
updated on every machine, the mount directory created and the filesystem
mounted.

   In many environments most people use the same few workstations, but
it is convenient to go to a colleague's machine and access your own
files.  When a server goes down, it can cause a process on a client
machine to hang.  By minimizing the mounted filesystems to only include
those actively being used, there is less chance that a filesystem will
be mounted when a server goes down.

   The following is a short extract from a map taken from a research
fileserver at Imperial College.

   Note the entry for `localhost' which is used for users such as the
operator (`opr') who have a home directory on most machine as
`/home/localhost/opr'.

     /defaults       opts:=rw,intr,grpid,nosuid
     charm           host!=${key};type:=nfs;rhost:=${key};rfs:=/home/${key} \
                     host==${key};type:=ufs;dev:=/dev/xd0g
     #
     ...
     
     #
     localhost       type:=link;fs:=${host}
     ...
     #
     # dylan has two user disks so have a
     # top directory in which to mount them.
     #
     dylan           type:=auto;fs:=${map};pref:=${key}/
     #
     dylan/dk2       host!=dylan;type:=nfs;rhost:=dylan;rfs:=/home/${key} \
                     host==dylan;type:=ufs;dev:=/dev/dsk/2s0
     #
     dylan/dk5       host!=dylan;type:=nfs;rhost:=dylan;rfs:=/home/${key} \
                     host==dylan;type:=ufs;dev:=/dev/dsk/5s0
     ...
     #
     toytown         host!=${key};type:=nfs;rhost:=${key};rfs:=/home/${key} \
                     host==${key};type:=ufs;dev:=/dev/xy1g
     ...
     #
     zebedee         host!=${key};type:=nfs;rhost:=${key};rfs:=/home/${key} \
                     host==${key};type:=ufs;dev:=/dev/dsk/1s0
     #
     # Just for access...
     #
     gould           type:=auto;fs:=${map};pref:=${key}/
     gould/staff     host!=gould;type:=nfs;rhost:=gould;rfs:=/home/${key}
     #
     gummo           host!=${key};type:=nfs;rhost:=${key};rfs:=/home/${key}
     ...

   This map is shared by most of the machines listed so on those
systems any of the user disks is accessible via a consistent name.  Amd
is started with the following command

     amd /home amd.home

   Note that when mounting a remote filesystem, the "automounted" mount
point is referenced, so that the filesystem will be mounted if it is
not yet (at the time the remote `mountd' obtains the file handle).


File: am-utils.info,  Node: Home Directories,  Next: Architecture Sharing,  Prev: User Filesystems,  Up: Examples

Home Directories
================

   One convention for home directories is to locate them in `/homes' so
user `jsp''s home directory is `/homes/jsp'.  With more than a single
fileserver it is convenient to spread user files across several
machines.  All that is required is a mount-map which converts login
names to an automounted directory.

   Such a map might be started by the command:

     amd /homes amd.homes

   where the map `amd.homes' contained the entries:

     /defaults   type:=link   # All the entries are of type:=link
     jsp         fs:=/home/charm/jsp
     njw         fs:=/home/dylan/dk5/njw
     ...
     phjk        fs:=/home/toytown/ai/phjk
     sjv         fs:=/home/ganymede/sjv

   Whenever a login name is accessed in `/homes' a symbolic link
appears pointing to the real location of that user's home directory.  In
this example, `/homes/jsp' would appear to be a symbolic link pointing
to `/home/charm/jsp'.  Of course, `/home' would also be an automount
point.

   This system causes an extra level of symbolic links to be used.
Although that turns out to be relatively inexpensive, an alternative is
to directly mount the required filesystems in the `/homes' map.  The
required map is simple, but long, and its creation is best automated.
The entry for `jsp' could be:

     jsp   -sublink:=${key};rfs:=/home/charm \
                    host==charm;type:=ufs;dev:=/dev/xd0g \
                    host!=charm;type:=nfs;rhost:=charm

   This map can become quite big if it contains a large number of
entries.  By combining two other features of Amd it can be greatly
simplified.

   First the UFS partitions should be mounted under the control of
`/etc/fstab', taking care that they are mounted in the same place that
Amd would have automounted them.  In most cases this would be something
like `/a/"host"/home/"host"' and `/etc/fstab' on host `charm' would
have a line:

     /dev/xy0g /a/charm/home/charm 4.2 rw,nosuid,grpid 1 5

   The map can then be changed to:

     /defaults    type:=nfs;sublink:=${key};opts:=rw,intr,nosuid,grpid
     jsp          rhost:=charm;rfs:=/home/charm
     njw          rhost:=dylan;rfs:=/home/dylan/dk5
     ...
     phjk         rhost:=toytown;rfs:=/home/toytown;sublink:=ai/${key}
     sjv          rhost:=ganymede;rfs:=/home/ganymede

   This map operates as usual on a remote machine (ie `${host}' not
equal to `${rhost}').  On the machine where the filesystem is stored
(ie `${host}' equal to `${rhost}'), Amd will construct a local
filesystem mount point which corresponds to the name of the locally
mounted UFS partition.  If Amd is started with the `-r' option then
instead of attempting an NFS mount, Amd will simply inherit the UFS
mount (*note Inheritance Filesystem::.).  If `-r' is not used then a
loopback NFS mount will be made.  This type of mount is known to cause
a deadlock on many systems.


File: am-utils.info,  Node: Architecture Sharing,  Next: Wildcard Names,  Prev: Home Directories,  Up: Examples

Architecture Sharing
====================

   Often a filesystem will be shared by machines of different
architectures.  Separate trees can be maintained for the executable
images for each architecture, but it may be more convenient to have a
shared tree, with distinct subdirectories.

   A shared tree might have the following structure on the fileserver
(called `fserver' in the example):

     local/tex
     local/tex/fonts
     local/tex/lib
     local/tex/bin
     local/tex/bin/sun3
     local/tex/bin/sun4
     local/tex/bin/hp9000
     ...

   In this example, the subdirectories of `local/tex/bin' should be
hidden when accessed via the automount point (conventionally `/vol').
A mount-map for `/vol' to achieve this would look like:

     /defaults   sublink:=${/key};rhost:=fserver;type:=link
     tex         type:=auto;fs:=${map};pref:=${key}/
     tex/fonts   host!=fserver;type:=nfs;rfs:=/vol/tex \
                 host==fserver;fs:=/usr/local/tex
     tex/lib     host!=fserver;type:=nfs;rfs:=/vol/tex \
                 host==fserver;fs:=/usr/local/tex
     tex/bin     -sublink:=${/key}/${arch} \
                 host!=fserver;type:=nfs;rfs:=/vol/tex \
                 host:=fserver;fs:=/usr/local/tex

   When `/vol/tex/bin' is referenced, the current machine architecture
is automatically appended to the path by the `${sublink}' variable.
This means that users can have `/vol/tex/bin' in their `PATH' without
concern for architecture dependencies.


File: am-utils.info,  Node: Wildcard Names,  Next: rwho servers,  Prev: Architecture Sharing,  Up: Examples

Wildcard Names & Replicated Servers
===================================

   By using the wildcard facility, Amd can "overlay" an existing
directory with additional entries.  The system files are usually
mounted under `/usr'.  If instead, Amd is mounted on `/usr', additional
names can be overlayed to augment or replace names in the "master"
`/usr'.  A map to do this would have the form:

     local  type:=auto;fs:=local-map
     share  type:=auto;fs:=share-map
     *      -type:=nfs;rfs:=/export/exec/${arch};sublink:="${key}" \
             rhost:=fserv1  rhost:=fserv2  rhost:=fserv3

   Note that the assignment to `${sublink}' is surrounded by double
quotes to prevent the incoming key from causing the map to be
misinterpreted.  This map has the effect of directing any access to
`/usr/local' or `/usr/share' to another automount point.

   In this example, it is assumed that the `/usr' files are replicated
on three fileservers: `fserv1', `fserv2' and `fserv3'.  For any
references other than to `local' and `share' one of the servers is used
and a symbolic link to ${autodir}/${rhost}/export/exec/${arch}/whatever
is returned once an appropriate filesystem has been mounted.


File: am-utils.info,  Node: rwho servers,  Next: /vol,  Prev: Wildcard Names,  Up: Examples

`rwho' servers
==============

   The `/usr/spool/rwho' directory is a good candidate for automounting.
For efficiency reasons it is best to capture the rwho data on a small
number of machines and then mount that information onto a large number
of clients.  The data written into the rwho files is byte order
dependent so only servers with the correct byte ordering can be used by
a client:

     /defaults         type:=nfs
     usr/spool/rwho    -byte==little;rfs:=/usr/spool/rwho \
                           rhost:=vaxA  rhost:=vaxB \
                       || -rfs:=/usr/spool/rwho \
                           rhost:=sun4  rhost:=hp300


File: am-utils.info,  Node: /vol,  Next: /defaults with selectors,  Prev: rwho servers,  Up: Examples

`/vol'
======

   `/vol' is used as a catch-all for volumes which do not have other
conventional names.

   Below is part of the `/vol' map for the domain `doc.ic.ac.uk'.  The
`r+d' tree is used for new or experimental software that needs to be
available everywhere without installing it on all the fileservers.
Users wishing to try out the new software then simply include
`/vol/r+d/{bin,ucb}' in their path.

   The main tree resides on one host `gould.doc.ic.ac.uk', which has
different `bin', `etc', `lib' and `ucb' sub-directories for each
machine architecture.  For example, `/vol/r+d/bin' for a Sun-4 would be
stored in the sub-directory `bin/sun4' of the filesystem `/usr/r+d'.
When it was accessed a symbolic link pointing to
`/a/gould/usr/r+d/bin/sun4' would be returned.

     /defaults    type:=nfs;opts:=rw,grpid,nosuid,intr,soft
     wp           -opts:=rw,grpid,nosuid;rhost:=charm \
                  host==charm;type:=link;fs:=/usr/local/wp \
                  host!=charm;type:=nfs;rfs:=/vol/wp
     ...
     #
     src          -opts:=rw,grpid,nosuid;rhost:=charm \
                  host==charm;type:=link;fs:=/usr/src \
                  host!=charm;type:=nfs;rfs:=/vol/src
     #
     r+d          type:=auto;fs:=${map};pref:=r+d/
     # per architecture bin,etc,lib&ucb...
     r+d/bin      rhost:=gould.doc.ic.ac.uk;rfs:=/usr/r+d;sublink:=${/key}/${arch}
     r+d/etc      rhost:=gould.doc.ic.ac.uk;rfs:=/usr/r+d;sublink:=${/key}/${arch}
     r+d/include  rhost:=gould.doc.ic.ac.uk;rfs:=/usr/r+d;sublink:=${/key}
     r+d/lib      rhost:=gould.doc.ic.ac.uk;rfs:=/usr/r+d;sublink:=${/key}/${arch}
     r+d/man      rhost:=gould.doc.ic.ac.uk;rfs:=/usr/r+d;sublink:=${/key}
     r+d/src      rhost:=gould.doc.ic.ac.uk;rfs:=/usr/r+d;sublink:=${/key}
     r+d/ucb      rhost:=gould.doc.ic.ac.uk;rfs:=/usr/r+d;sublink:=${/key}/${arch}
     # hades pictures
     pictures     -opts:=rw,grpid,nosuid;rhost:=thpfs \
                  host==thpfs;type:=link;fs:=/nbsd/pictures \
                  host!=thpfs;type:=nfs;rfs:=/nbsd;sublink:=pictures
     # hades tools
     hades        -opts:=rw,grpid,nosuid;rhost:=thpfs \
                  host==thpfs;type:=link;fs:=/nbsd/hades \
                  host!=thpfs;type:=nfs;rfs:=/nbsd;sublink:=hades
     # bsd tools for hp.
     bsd          -opts:=rw,grpid,nosuid;arch==hp9000;rhost:=thpfs \
                  host==thpfs;type:=link;fs:=/nbsd/bsd \
                  host!=thpfs;type:=nfs;rfs:=/nbsd;sublink:=bsd


File: am-utils.info,  Node: /defaults with selectors,  Next: /tftpboot in a chroot-ed environment,  Prev: /vol,  Up: Examples

`/defaults' with selectors
==========================

   It is sometimes useful to have different defaults for a given map.
To achieve this, the `/defaults' entry must be able to process normal
selectors.  This feature is turned on by setting `selectors_on_default
= yes' in the `amd.conf' file.  *Note selectors_on_default Parameter::.

   In this example, I set different default NFS mount options for hosts
which are running over a slower network link.  By setting a smaller size
for the NFS read and write buffer sizes, you can greatly improve remote
file service performance.

     /defaults \
       wire==slip-net;opts:=rw,intr,rsize=1024,wsize=1024,timeo=20,retrans=10 \
       wire!=slip-net;opts:=rw,intr


File: am-utils.info,  Node: /tftpboot in a chroot-ed environment,  Prev: /defaults with selectors,  Up: Examples

`/tftpboot' in a chroot-ed environment
======================================

   In this complex example, we attempt to run an Amd process *inside* a
chroot-ed environment.  `tftpd' (Trivial FTP) is used to trivially
retrieve files used to boot X-Terminals, Network Printers, Network
routers, diskless workstations, and other such devices.  For security
reasons, `tftpd' (and also `ftpd') processes are run using the
chroot(2) system call.  This provides an environment for these
processes, where access to any files outside the directory where the
chroot-ed process runs is denied.

   For example, if you start `tftpd' on your system with

     chroot /tftpboot /usr/sbin/tftpd

then the `tftpd' process will not be able to access any files outside
`/tftpboot'.  This ensures that no one can retrieve files such as
`/etc/passwd' and run password crackers on it.

   Since the TFTP service works by broadcast, it is necessary to have at
least one TFTP server running on each subnet.  If you have lots of files
that you need to make available for `tftp', and many subnets, it could
take significant amounts of disk space on each host serving them.

   A solution we implemented at Columbia University was to have every
host run `tftpd', but have those servers retrieve the boot files from
two replicated servers.  Those replicated servers have special
partitions dedicated to the many network boot files.

   We start Amd as follows:

     amd /tftpboot/.amd amd.tftpboot

   That is, Amd is serving the directory `/tftpboot/.amd'.  The `tftp'
server runs inside `/tftpboot' and is chroot-ed in that directory too.
The `amd.tftpboot' map looks like:

     #
     # Amd /tftpboot directory -> host map
     #
     
     /defaults  opts:=nosuid,ro,intr,soft;fs:=/tftpboot/import;type:=nfs
     
     tp         host==lol;rfs:=/n/lol/import/tftpboot;type:=lofs \
                host==ober;rfs:=/n/ober/misc/win/tftpboot;type:=lofs \
                rhost:=ober;rfs:=/n/ober/misc/win/tftpboot \
                rhost:=lol;rfs:=/n/lol/import/tftpboot

   To help understand this example, I list a few of the file entries
that are created inside `/tftpboot':

     $ ls -la /tftpboot
     dr-xr-xr-x   2 root   512 Aug 30 23:11 .amd
     drwxrwsr-x  12 root   512 Aug 30 08:00 import
     lrwxrwxrwx   1 root    33 Feb 27  1997 adminpr.cfg -> ./.amd/tp/hplj/adminpr.cfg
     lrwxrwxrwx   1 root    22 Dec  5  1996 tekxp -> ./.amd/tp/xterms/tekxp
     lrwxrwxrwx   1 root     1 Dec  5  1996 tftpboot -> .

   Here is an explanation of each of the entries listed above:

`.amd'
     This is the Amd mount point.  Note that you do not need to run a
     separate Amd process for the TFTP service.  The chroot(2) system
     call only protects against file access, but the same process can
     still serve files and directories inside and outside the chroot-ed
     environment, because Amd itself was not run in chroot-ed mode.

`import'
     This is the mount point where Amd will mount the directories
     containing the boot files.  The map is designed so that remote
     directories will be NFS mounted (even if they are already mounted
     elsewhere), and local directories are loopback mounted (since they
     are not accessible outside the chroot-ed `/tftpboot' directory).

`adminpr.cfg'
`tekxp'
     Two manually created symbolic links to directories *inside* the
     Amd-managed directory.  The crossing of the component `tp' will
     cause Amd to automount one of the remote replicas.  Once crossed,
     access to files inside proceeds as usual.  The `adminpr.cfg' is a
     configuration file for an HP Laser-Jet 4si printer, and the `tekxp'
     is a directory for Tektronix X-Terminal boot files.

`tftpboot'
     This innocent looking symlink is important.  Usually, when devices
     boot via the TFTP service, they perform the `get file' command to
     retrieve FILE.  However, some devices assume that `tftpd' does not
     run in a chroot-ed environment, but rather "unprotected", and thus
     use a full pathname for files to retrieve, as in `get
     /tftpboot/file'.  This symlink effectively strips out the leading
     `/tftpboot/'.


File: am-utils.info,  Node: Internals,  Next: Acknowledgments & Trademarks,  Prev: Examples,  Up: Top

Internals
*********

   Note that there are more error and logging messages possible than are
listed here.  Most of them are self-explanatory.  Refer to the program
sources for more details on the rest.

* Menu:

* Log Messages::


File: am-utils.info,  Node: Log Messages,  Prev: Internals,  Up: Internals

Log Messages
============

   In the following sections a brief explanation is given of some of the
log messages made by Amd.  Where the message is in `typewriter' font,
it corresponds exactly to the message produced by Amd.  Words in
"italic" are replaced by an appropriate string.  Variables, `${var}',
indicate that the value of the appropriate variable is output.

   Log messages are either sent directly to a file, or logged via the
syslog(3) mechanism.  *Note log_file Parameter::.  In either case,
entries in the file are of the form:
     date-string  hostname amd[pid]  message

* Menu:

* Fatal errors::
* Info messages::


File: am-utils.info,  Node: Fatal errors,  Next: Info messages,  Prev: Log Messages,  Up: Log Messages

Fatal errors
------------

   Amd attempts to deal with unusual events.  Whenever it is not
possible to deal with such an error, Amd will log an appropriate
message and, if it cannot possibly continue, will either exit or abort.
These messages are selected by `-x fatal' on the command line.  When
syslog(3) is being used, they are logged with level `LOG_FATAL'.  Even
if Amd continues to operate it is likely to remain in a precarious
state and should be restarted at the earliest opportunity.

Attempting to inherit not-a-filesystem
     The prototype mount point created during a filesystem restart did
     not contain a reference to the restarted filesystem.  This error
     "should never happen".

Can't bind to domain "NIS-domain"
     A specific NIS domain was requested on the command line, but no
     server for that domain is available on the local net.

Can't determine IP address of this host (hostname)
     When Amd starts it determines its own IP address.  If this lookup
     fails then Amd cannot continue.  The hostname it looks up is that
     obtained returned by gethostname(2) system call.

Can't find root file handle for automount point
     Amd creates its own file handles for the automount points.  When it
     mounts itself as a server, it must pass these file handles to the
     local kernel.  If the filehandle is not obtainable the mount point
     is ignored.  This error "should never happen".

Must be root to mount filesystems (euid = euid)
     To prevent embarrassment, Amd makes sure it has appropriate system
     privileges.  This amounts to having an euid of 0.  The check is
     made after argument processing complete to give non-root users a
     chance to access the `-v' option.

No work to do - quitting
     No automount points were given on the command line and so there is
     no work to do.

Out of memory
     While attempting to malloc some memory, the memory space available
     to Amd was exhausted.  This is an unrecoverable error.

Out of memory in realloc
     While attempting to realloc some memory, the memory space
     available to Amd was exhausted.  This is an unrecoverable error.

cannot create rpc/udp service
     Either the NFS or AMQ endpoint could not be created.

gethostname: description
     The gethostname(2) system call failed during startup.

host name is not set
     The gethostname(2) system call returned a zero length host name.
     This can happen if Amd is started in single user mode just after
     booting the system.

ifs_match called!
     An internal error occurred while restarting a pre-mounted
     filesystem.  This error "should never happen".

mount_afs: description
     An error occurred while Amd was mounting itself.

run_rpc failed
     Somehow the main NFS server loop failed.  This error "should never
     happen".

unable to free rpc arguments in amqprog_1
     The incoming arguments to the AMQ server could not be free'ed.

unable to free rpc arguments in nfs_program_1
     The incoming arguments to the NFS server could not be free'ed.

unable to register (AMQ_PROGRAM, AMQ_VERSION, udp)
     The AMQ server could not be registered with the local portmapper
     or the internal RPC dispatcher.

unable to register (NFS_PROGRAM, NFS_VERSION, 0)
     The NFS server could not be registered with the internal RPC
     dispatcher.

   XXX: This section needs to be updated

